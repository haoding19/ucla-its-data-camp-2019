{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCLA ITS Data Camp, Day 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm-Up / Review Exercise: Get Scooter Data\n",
    "You may have seen some scooters around UCLA. What you may not have known is that the data on dockless mobility in Los Angeles is made publicly available via APIs provided by each company, which you can read more about [here](https://medium.com/tim-black/scooters-mapped-55a2afca46b1). Each company provides a separate URL for each city they operate in. I've compiled a master list of the scooter companies' GBFS feeds [here](https://github.com/black-tea/swarm-of-scooters/blob/master/data/systems.csv); for example, you will see that the Los Angeles region includes 7 different feeds across 6 providers. \n",
    "\n",
    "Using our new skills at getting data from APIs, let's practice grabbing data from these companies. Build a function that will take in a URL as an argument and will output a JSON file with the response content. \n",
    "\n",
    "_Function input:_ URL of GBFS data feed  \n",
    "_Function output:_ Writes JSON to `data/raw` with filename format: `scooter_[%Y-%m-%d-%H-%M-%S].json` (no return value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build a function that takes in a url and writes out the data to your `data/raw` directory.\n",
    "#       Hint: Don't forget to import the necessary packages!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick any of the URLs on [this page](https://github.com/black-tea/swarm-of-scooters/blob/master/data/systems.csv) to test out your function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add URL\n",
    "scooter_url = \n",
    "\n",
    "# Run your function to download the data and store for later\n",
    "get_scooter_gbfs(scooter_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling, Part 1\n",
    "For today's exercises, we are going to be working with data using Python's `pandas` package. We will start with how to read in CSV data, then how to combine multiple dataframes into one. We will then move on to learning about how to calculate summary statistics, join data from different dataframes, and produce a summary extract for export. \n",
    "\n",
    "### Exercise 1: Importing & Concatenating Data in Pandas\n",
    "Yesterday we prepared a series of CSV file extracts containing Metro vehicle location data and saved them to disk. Today we will learn how to read in CSV files using `pandas.read_csv()` instead of the `csv` package. We will then combine multiple dataframes into one dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries (pandas)\n",
    "import pandas as pd\n",
    "\n",
    "# TODO: Add the paths to any two of your CSV files from yesterday\n",
    "filepath_1 = \n",
    "filepath_2 = \n",
    "\n",
    "# TODO: Read both CSVs like this -> pd.read_csv(filepath)\n",
    "metro_df1 = \n",
    "metro_df2 = \n",
    "\n",
    "# Here is how we get the number of rows in a dataframe\n",
    "print(len(metro_df1))\n",
    "print(len(metro_df2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two dataframes with the same columns can be combined using [pd.concat](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html). Let's give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the dataframes\n",
    "metro_df = pd.concat([metro_df1, metro_df2])\n",
    "\n",
    "# TODO: Print the length of the new combined dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've certainly cut down on the amount of code by moving to `pd.read_csv()`. However, do we still need to write out every filepath and read each one in individually? What if we built a loop to read through all of our files and append them together automatically? \n",
    "  \n",
    "Let's take a stab at it. We are going to use Python's [glob](https://docs.python.org/3/library/glob.html) package to search for all of our la metro data extracts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# This handy statement will get us a list of filenames\n",
    "all_filenames = [i for i in glob.glob('data/raw/lametro*.csv')]\n",
    "\n",
    "# Create an empty list that will hold our dataframes\n",
    "df_list = []\n",
    "\n",
    "# TODO: Loop through all of our `all_filenames`, read_csv to convert to a dataframe, then append the df to df_list\n",
    "\n",
    "# TODO: Concatenate all the dataframes together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many rows we have in our new SUPER DATAFRAME\n",
    "print(len(metro_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Summarize Collision Data (Fatalities by Year)\n",
    " The [Los Angeles Vision Zero Safety Study](https://view.joomag.com/vision-zero-safety-study/0065798001485405769?short), published in January 2017, provides insights about collisions in the City using data from 2009-2013. Let's attempt to replicate some the analysis, which will allow us to begin answering questions such as these: Are we making progress towards that goal? If not, how far behind are we?  \n",
    "\n",
    "##### Prep\n",
    "Let's start by reading in the CSV files for collision data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import each of the three tables\n",
    "collisions = \n",
    "parties = \n",
    "victims = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at the data we just imported. It is a good habit to build to constantly check the data as you move it through the wrangling process to make sure that it is not getting transformed in unexpected ways. We can take a look at the first couple of rows of a dataframe by using Pandas' [df.head()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html) method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the 'head' of the collision table\n",
    "collisions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do all of these feature values represent? You can take a look at the [SWITRS Codebook](https://peteraldhous.com/Data/ca_traffic/SWITRS_codebook.pdf), which will explain each variable in all three of our tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, we can look at the 'tail' of the table by running the code below\n",
    "parties.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # TODO: Change the size of the head to show the top 15 rows (see documentation above)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are accessing really WIDE tables like the ones above, the `df.head()` method won't show us all the column names. We can access the column names by calling `df.columns` to get the list of all columns in the table. Access the [SWITRS Codebook](https://peteraldhous.com/Data/ca_traffic/SWITRS_codebook.pdf) for detail on what all these column values represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print the column names for the collisions table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many rows are in each dataset? For each of the tables, print out the following: `The number of rows in the [table name] is [number of rows]`. For example: `The number of rows in the collision table is 1200`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print out the number of rows for each of the three dataframes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Filter for fatalities\n",
    "The first step to summarazing fatalites by year is to employ a row-based filter to our data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Filter for only collisions\n",
    "fatal_collisions = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many fatal collisions occurred between 2009 and 2013?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print out the number of fatal collisions (Hint: It is the rowcount, since we filtered for only fatal collisions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Count fatalities by year\n",
    "An important distinction to be aware of when reading the report and replicating the findings is whether the statistic refers to _collisons_ or to _victims_ since this will determine which table we will be working with. In this case, since we are benchmarking our progress against the goal of zero _fatalities_ we will be looking at the victim table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Within fatal_collisions, group by year and count\n",
    "fatalities_by_year = \n",
    "\n",
    "# Let's take a look\n",
    "print(fatalities_by_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Prepare and Export a Custom Data Extract\n",
    "While the Jupyter notebook is in most cases the ideal place to explore and present data, we occassionaly want to be able to export data or findings for use elsewhere. After we take a look at the trends for traffic fatalities, we are going to prepare a custom data extract.\n",
    "\n",
    "It is not uncommon for policymakers to want to know more specific information for those who have been killed in a traffic collision. Let's pretend the Mayor has asked for a list of all fatalities in the year 2013 with the following information for each fatality:\n",
    "1. Date & time of collision - Collision Table\n",
    "2. Type of collision (Head-On, Sideswipe, etc.) - Collision Table\n",
    "3. Whether Alcohol was involved (yes/no) - Collision Table\n",
    "4. Age of the victim - Victim Table\n",
    "5. Victim's mode of transportation - Victim Table\n",
    "6. Distance of the collision from the intersection (to know whether it was intersection-related) - Collision Table\n",
    "\n",
    "Let's produce this data extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Filter for 2013 fatalities\n",
    "We can begin this assignment by noting that we are only interested in fatalities within 2013. We will therefore be applying 2 filters:  \n",
    "- Year = 2013 and \n",
    "- Victim Degree of Injury = 1 (fatal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Filter for 2013 victims\n",
    "victims_2013 = \n",
    "\n",
    "# TODO: Filter 2013 fatal victims \n",
    "fatal_victims_2013 = \n",
    "\n",
    "# Examine what's left\n",
    "fatal_victims_2013.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many fatalities were in 2013? How is this number different from the count of fatal _collisions?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print out the number of total number of victims of fatal injury for 2013 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Join tables  \n",
    "Looking through the SWITRS Codebook, you will notice that these data are not contained within a single table - instead we are going to need to join at least 2 tables to produce the necessary information. For each of the requested fields, find it in the codebook to determine the table the field resides in; you will then want to make an **_inner join_** (see [here](http://www.khankennels.com/blog/index.php/archives/2007/04/20/getting-joins/) for a summary of join types) with the currently filtered fatalies data.  \n",
    "\n",
    "![joins](join_types.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Join the necessary tables using pd.merge\n",
    "merged_fatal_victims_2013 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Filter out un-necessary columns\n",
    "After joining both of the tables, you will notice that we end up with a large number of columns. We don't want to make the policymakers hunt through the data extract for the necessary information; instead, we should be filtering for only the columns that were requested in the extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Let's make a list of all the columns we want to keep\n",
    "keep_cols = \n",
    "\n",
    "# TODO: Filter the df for those columns we want\n",
    "output_fatal_victims_2013 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: Write-out the CSV\n",
    "We can write out a CSV using the syntax: `df.to_csv(filepath)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before writing out, let's confirm that our current df matches the desired output format,\n",
    "# using panda's .head() method from before\n",
    "output_fatal_victims_2013.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write out the CSV to the data/processed directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Challenge Exercise\n",
    "Prepare the same extract, but this time include the following additional fields:\n",
    "- Whether the victim was at fault..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
